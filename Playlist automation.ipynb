{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BHT Data Applications project\n",
    "# Automatic Anime recommendation Algorithm\n",
    "### This project aims to create an algorithm that can determine what anime to recommend to a user.\n",
    "##### Authors: Rashmi Di Michino and Antonin Mathubert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 320000 users and 16000 animes dataset was taken from https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020 <br>\n",
    "We are going to use this dataset to build a model that can recommend an anime based on the animes that the user is watching, has dropped, has kept on hold or put on their watching list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing and parsing the data\n",
    "First, we want to import all of our available data in a suitable manner so it is treatable for the next steps of the project.<br><br>\n",
    "In order to load the data, we are going to do it by chunking the csv file so it's more efficient. Then we're changing the default type of the columns to be more convenient memory wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from itertools import combinations\n",
    "import plotly.graph_objs as go\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/rashm/OneDrive/Desktop/data_applications_project/julius/anime_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/anime/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chunks = pd.read_csv(path+\"animelist.csv\", chunksize=20000)\n",
    "animes_df = pd.read_csv(path+\"anime.csv\")\n",
    "\n",
    "\n",
    "chunks = []\n",
    "for chunk in dataset_chunks:\n",
    "    chunks.append(chunk)\n",
    "    \n",
    "dataset = pd.concat(chunks, ignore_index=True)\n",
    "dataset = dataset.astype({'user_id': \"int32\", 'anime_id': 'int32', \"watching_status\": \"int16\", \"rating\": \"int16\"})\n",
    "\n",
    "dataset_chunks = None\n",
    "chunks = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recommendation system based on the watched animes\n",
    "In this first version we're going to implement a recommendation system based on which animes the users have seen, for example if someone has watched cowboy bepop, they're going to be recommended to see death note\n",
    "#### Reducing the dataset\n",
    "As the dataset we're working with is too large, we're going to reduce it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['watched_episodes'], axis=1, inplace=True)\n",
    "dataset = dataset[(dataset['anime_id'] > 1) & (dataset['user_id'] < 20000)]\n",
    "dataset = dataset[(dataset['user_id'] != 61960) & (dataset['watching_status'] != 4)]\n",
    "dataset = dataset.drop(\"watching_status\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a sample of how the dataset is now structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset.head(10))\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to include only the first 1000 most watched animes\n",
    "top_animes = dataset['anime_id'].value_counts().nlargest(250).index\n",
    "df = dataset[dataset['anime_id'].isin(top_animes)]\n",
    "anime_counts = df[\"anime_id\"].value_counts()\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Group by user_id\n",
    "user_groups = df.groupby('user_id')\n",
    "\n",
    "# Create edges for each user's watched animes with ratings as weights\n",
    "for user_id, group in tqdm(user_groups):\n",
    "\tanimes = group['anime_id'].tolist()\n",
    "\tratings = group['rating'].tolist()\n",
    "\tfor (anime1, rating1), (anime2, rating2) in combinations(zip(animes, ratings), 2):\n",
    "\t\tif G.has_edge(anime1, anime2):\n",
    "\t\t\tG[anime1][anime2]['weight'] += (rating1 + rating2) / 20\n",
    "\t\telse:\n",
    "\t\t\tG.add_edge(anime1, anime2, weight=(rating1 + rating2) / 20)\n",
    "            \n",
    "\n",
    "\n",
    "# Get the top 5 most frequent links\n",
    "top_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)[:250]\n",
    "\n",
    "# Create a new graph with only the top 5 edges\n",
    "top_G = nx.Graph()\n",
    "top_G.add_edges_from([(u, v, {'weight': d['weight']}) for u, v, d in top_edges])\n",
    "\n",
    "# Normalize the weights between 0 and 5\n",
    "weights = [d['weight'] for u, v, d in top_G.edges(data=True)]\n",
    "min_weight = min(weights)\n",
    "max_weight = max(weights)\n",
    "\n",
    "min_count = anime_counts.min()\n",
    "max_count = anime_counts.max()\n",
    "\n",
    "node_sizes = [\n",
    "\t10 + (anime_counts[node] - min_count) * (50 - 10) / (max_count - min_count)\n",
    "\tfor node in top_G.nodes()\n",
    "]\n",
    "\n",
    "for u, v, d in top_G.edges(data=True):\n",
    "\td['normalized_weight'] = 5 * (d['weight'] - min_weight) / (max_weight - min_weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get positions for all nodes\n",
    "pos = nx.spring_layout(top_G, k=10, iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_texts = []\n",
    "for node in top_G.nodes():\n",
    "    neighbors = list(top_G[node])\n",
    "    weights = [top_G[node][neighbor]['weight'] for neighbor in neighbors]\n",
    "    total_weight = sum(weights)\n",
    "    percentages = [(neighbor, top_G[node][neighbor]['weight'] / total_weight * 100) for neighbor in neighbors]\n",
    "    percentages = sorted(percentages, key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    hover_text = f\"{animes_df[animes_df['MAL_ID'] == node]['English name'].values[0]}<br>\" + \"<br>\".join([f\"{animes_df[animes_df['MAL_ID'] == neighbor]['English name'].values[0]}: {weight:.2f}%\" for neighbor, weight in percentages])\n",
    "    hover_texts.append(hover_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edge traces\n",
    "edge_trace = []\n",
    "for edge in top_G.edges(data=True):\n",
    "\tx0, y0 = pos[edge[0]]\n",
    "\tx1, y1 = pos[edge[1]]\n",
    "\ttrace = go.Scatter(\n",
    "\t\tx=[x0, x1, None],\n",
    "\t\ty=[y0, y1, None],\n",
    "\t\tline=dict(width=edge[2]['normalized_weight'], color='gray'),  # Adjusted width for better visualization\n",
    "\t\thoverinfo=\"none\",\n",
    "\t\tmode='lines'\n",
    "\t)\n",
    "\tedge_trace.append(trace)\n",
    "\n",
    "# Create node trace\n",
    "node_trace = go.Scatter(\n",
    "\tx=[pos[node][0] for node in top_G.nodes()],\n",
    "\ty=[pos[node][1] for node in top_G.nodes()],\n",
    "\thovertext=hover_texts,\n",
    "\ttext=[animes_df[animes_df['MAL_ID'] == node]['English name'].values[0] for node in top_G.nodes()],\n",
    "\tmode='markers+text',\n",
    "\ttextposition='top center',\n",
    "\tmarker=dict(\n",
    "\t\tsize=node_sizes,\n",
    "\t\tcolor='skyblue',\n",
    "\t\tline=dict(width=2, color='black')\n",
    "\t)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure\n",
    "fig = go.Figure(\n",
    "\tdata=edge_trace + [node_trace],\n",
    "\tlayout=go.Layout(\n",
    "\t\ttitle='Top 5 Most Frequent Anime Watching Links',\n",
    "\t\ttitlefont_size=16,\n",
    "\t\tshowlegend=False,\n",
    "\t\thovermode='closest',\n",
    "\t\theight=800,\n",
    "\t\tmargin=dict(b=20, l=5, r=5, t=40),\n",
    "\t\tannotations=[dict(\n",
    "\t\t\ttext=\"\",\n",
    "\t\t\tshowarrow=False,\n",
    "\t\t\txref=\"paper\", yref=\"paper\"\n",
    "\t\t)],\n",
    "\t\txaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "\t\tyaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "\t)\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transforming the data into binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.pivot(index='user_id', columns='anime_id', values='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now converting our matrix into a binary matrix in order to be able to retrieve the association rules: we only take into account the ratings that are above 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset > 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving the association rules\n",
    "Finally, we are exploiting the mlxtend library to build the recommendation system and we're retrieving the association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets  = apriori(dataset, use_colnames=True, min_support=0.15) #Getting under 0.1 support takes too much computation time / memory and lacks of meaning.\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: [x for x in x])\n",
    "rules[\"consequents\"] = rules[\"consequents\"].apply(lambda x: [x for x in x])\n",
    "rules = rules[rules[\"confidence\"] > 0.2].reset_index().drop(\"index\", axis=1).sort_values(\"lift\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the rules detected by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rules.head(20))\n",
    "print(f\"{len(rules)} rules found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the rules\n",
    "These functions are designed to parse and filter the results of the detected rules, so we can understand them more easily.\n",
    "\n",
    "<hr>\n",
    "\n",
    "```find_recommendations_precise``` will compute every possible combination of the watched anime ids, and try to find them in the rules dataset.\n",
    "<hr>\n",
    "\n",
    "```find_recommendations_free``` will look for every occurence of each anime id in the rules, even if the antecedents frozen set isn't containing only the given id. \n",
    "\n",
    "It will return one dataset per seen anime, ordered from highest to lowest weight. Weight is computed by adding every confidence value obtained in the rules containing the recommendated anime in the consequents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combinations(ids):\n",
    "\tresult = []\n",
    "\tfor r in range(1, len(ids) + 1):\n",
    "\t\tpermutations = itertools.permutations(ids, r)\n",
    "\t\tfor p in permutations:\n",
    "\t\t\tresult.append(list(p))\n",
    "\n",
    "\tprint(f\"Found {len(result)} possible combinations.\")\n",
    "\treturn result\n",
    "\n",
    "def find_recommendations_precise(anime_ids):\n",
    "\trecommendations = []\n",
    "\t\n",
    "\tfor combination in tqdm(generate_combinations(anime_ids), desc=\"Trying every possible combination...\"):\n",
    "\t\tfilter_df = rules[\"antecedents\"].apply(lambda x: x == combination) & rules[\"consequents\"].apply(lambda x: np.all([id not in x for id in anime_ids]))\n",
    "\t\tif filter_df.apply(lambda x: x != False).sum() < 1:\n",
    "\t\t\tcontinue\n",
    "\t\trecommendation = (combination, rules[filter_df][\"consequents\"].values, rules[filter_df][\"confidence\"].values, rules[filter_df][\"lift\"].values)\n",
    "\t\trecommendations.append(recommendation)\n",
    "\n",
    "\treturn sorted(recommendations, key=lambda x: x[3], reverse=True)\n",
    "\n",
    "def find_recommendations_free(anime_ids):\n",
    "\trecommendations = []\n",
    "\n",
    "\tfor id in anime_ids:\n",
    "\t\tfilter_df = rules[\"antecedents\"].apply(lambda x: id in x) & rules[\"consequents\"].apply(lambda x: np.all([id not in x for id in anime_ids]))\n",
    "\t\tif filter_df.apply(lambda x: x != False).sum() < 1:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\trecommendation = pd.DataFrame({\"source\": id, \"antecedents\": rules[filter_df][\"antecedents\"].values, \"consequents\": rules[filter_df][\"consequents\"].values, \"confidence\": rules[filter_df][\"confidence\"].values, \"lift\": rules[filter_df][\"lift\"].values})\n",
    "\t\trecommendations.append(recommendation)\n",
    "\n",
    "\trecommendations = pd.concat(recommendations)\n",
    "\trecommendations_dict = {anime: {}  for anime in anime_ids}\n",
    "\trecommendations_df = []\n",
    "\tfor anime in recommendations_dict:\n",
    "\t\trows = recommendations[recommendations[\"source\"] == anime]\n",
    "\t\tfor _, row in rows.iterrows():\n",
    "\t\t\tfor x in row[\"consequents\"]:\n",
    "\t\t\t\tif x in recommendations_dict[anime]:\n",
    "\t\t\t\t\trecommendations_dict[anime][x] += row[\"lift\"] * row[\"confidence\"]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\trecommendations_dict[anime][x] = row[\"lift\"]* row[\"confidence\"]\n",
    "\t\t\n",
    "\t\tfor anime_recommended in recommendations_dict[anime]:\n",
    "\t\t\trecommendations_df.append([animes_df[animes_df[\"MAL_ID\"] == anime][\"English name\"].values[0], animes_df[animes_df[\"MAL_ID\"] == anime_recommended][\"English name\"].values[0], recommendations_dict[anime][anime_recommended]])\n",
    "\n",
    "\trecommendations_df = pd.DataFrame(recommendations_df, columns=['source', 'recommended_id', 'weight']).sort_values(by=\"weight\", ascending=False).groupby(\"source\")\n",
    "\n",
    "\treturn recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the previously defined function and parse the results to print them and link them with the anime infos dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_animes = [23273] #More than 7 at a time takes forever.\n",
    "print(seen_animes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recommendations in find_recommendations_precise(seen_animes):\n",
    "\tfor i in range(len(recommendations[1])):\n",
    "\t\trecommendation = (recommendations[0], recommendations[1][i], recommendations[2][i], recommendations[3][i])\n",
    "\t\tprint(\"Because you have seen %s, we think you would like %s with %.3f%% confidence. You are also %.3f%% more likely to watch this/these anime(s).\" % (\n",
    "\t\t\t\" and \".join([animes_df[animes_df[\"MAL_ID\"] == x][\"English name\"].values[0] + f\" ({str(x)})\" for x in recommendation[0]]), \n",
    "\t\t\t\" and \".join([animes_df[animes_df[\"MAL_ID\"] == x][\"English name\"].values[0] + f\" ({str(x)})\" for x in recommendation[1]]), \n",
    "\t\t\trecommendation[2] * 100,\n",
    "\t\t\trecommendation[3] * 100 - 100)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part needs improvement, because the results are not consise enough. \n",
    "# We could create a list of every anime that has been watch by people who's seen a certain anime, and return the most common ones instead of every one. \n",
    "for index, recommendation_df in find_recommendations_free(seen_animes):\n",
    "\tdisplay(recommendation_df.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
