{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BHT Data Applications project\n",
    "# Automatic Anime recommendation Algorithm\n",
    "### This project aims to create an algorithm that can determine what anime to recommend to a user.\n",
    "##### Authors: Rashmi Di Michino and Antonin Mathubert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 320000 users and 16000 animes dataset was taken from https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020 <br>\n",
    "We are going to use this dataset to build a model that can recommend an anime based on the animes that the user is watching, has dropped, has kept on hold or put on their watching list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing and parsing the data\n",
    "First, we want to import all of our available data in a suitable manner so it is treatable for the next steps of the project.<br><br>\n",
    "Here, we are going to propose two methods to do that. The first one is very slow but the \"clean\" way to read json data. Because we don't actually need all the data from the files, we are going to use a second hand made method that is significantly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "files = os.listdir(\"dataset/data\")\n",
    "print(files[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1 \n",
    "~4h on i7 10th gen (8 cores)\n",
    "\n",
    "This method is the clean one. We use python libraries (pandas and json) that will parse ALL of the data for us, we just keep what is interesting.\n",
    "\n",
    "\n",
    "The down side of this method is that it is very slow because we have to parse 100% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data = \"dataset/data.json\"\n",
    "\n",
    "tracks_df = pd.read_json(saved_data)\n",
    "initial_skip = True\n",
    "\n",
    "for file in tqdm(files, desc=\"Importing files...\"):\n",
    "  if len(tracks_df) > 0 and initial_skip:\n",
    "    if file == tracks_df.tail(1)['Slice origin'].values[0]:\n",
    "      initial_skip = False\n",
    "    else:\n",
    "      continue\n",
    "\n",
    "  with open(os.path.join(\"dataset/data\", file), 'r') as json_file:\n",
    "    json_data = json.loads(json_file.read())\n",
    "    playlists = json_data[\"playlists\"]\n",
    "\n",
    "    file_df = []\n",
    "    for playlist in playlists:\n",
    "      playlist_df = pd.DataFrame(columns=['Track name', 'Track album', 'Track artist', 'Playlist name', 'Playlist pid', 'Slice origin'])\n",
    "\n",
    "      playlist_name = playlist[\"name\"]\n",
    "      playlist_pid = playlist[\"pid\"]\n",
    "      tracks = playlist[\"tracks\"]\n",
    "\n",
    "      for track in tracks:\n",
    "        playlist_df.loc[len(file_df)] = [track['track_name'], track['album_name'], track['artist_name'], playlist_name, playlist_pid, file]\n",
    "\n",
    "      file_df.append(playlist_df)    \n",
    "  tracks_df = pd.concat([tracks_df] + file_df, ignore_index=True)\n",
    "  tracks_df.to_json(saved_data, orient=\"records\", indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2\n",
    "~ 0.167h on i7 10th gen 8 cores\n",
    "\n",
    "This method is the fast one, and the one we used in our case. In this method, we don't need to parse the entirety of a file. We just focus on the data that we want to extract, and parse it according to our need.\n",
    "\n",
    "This is significantly faster because of we have to parse only ~50% of the data and it is not converted into a DataFrame until the end of a chunk save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/media/fiddle/Data/OneDrive - UniversitÃ© de Technologie de Troyes/Dev/Projets/Python/bht-data/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = []\n",
    "slice_i = 0\n",
    "\n",
    "def getStrValue(line):\n",
    "  return re.search(r'.*?: \"(.*)\"', line).group(1)\n",
    "\n",
    "def getIntValue(line):\n",
    "  return re.search(r'.*?: (.*)', line).group(1)\n",
    "\n",
    "for file in tqdm(files, desc=\"Importing files...\"):\n",
    "  with open(os.path.join(\"dataset/data\", file), 'r') as data_file:\n",
    "    data = data_file.read().split(\"\\n\")\n",
    "    playlist_name = \"\"\n",
    "    playlist_pid = \"\"\n",
    "\n",
    "    track_info = [False] * 3\n",
    "    \n",
    "    for line in data:\n",
    "      if '\"name\"' in line:\n",
    "        playlist_name = getStrValue(line)\n",
    "      \n",
    "      if '\"pid\"' in line:\n",
    "        playlist_pid = getIntValue(line)\n",
    "\n",
    "      if '\"artist_name\"' in line:\n",
    "        track_info[2] = getStrValue(line)\n",
    "\n",
    "      if '\"track_name\"' in line:\n",
    "        track_info[0] = getStrValue(line)\n",
    "\n",
    "      if '\"album_name\"' in line:\n",
    "        track_info[1] = getStrValue(line)\n",
    "\n",
    "      if not False in track_info:\n",
    "        tracks.append(track_info + [playlist_name, playlist_pid, str(file)])\n",
    "        track_info = [False] * 3\n",
    "        if len(tracks) > 2 ** 21: #Creating chunked data to be sure to have enough RAM for the whole process\n",
    "          tracks_df = pd.DataFrame(tracks, columns=['Track name', 'Track album', 'Track artist', 'Playlist name', 'Playlist pid', 'Slice origin'])\n",
    "          tracks_df.to_json(save_dir + f\"dataslice_{slice_i}.json\", orient=\"records\", indent=4)\n",
    "          slice_i += 1\n",
    "          tracks = []\n",
    "          tracks_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the converted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = os.listdir(save_dir)\n",
    "main_df = pd.DataFrame(columns=[\"Track name\", \"Track album\", \"Track artist\", \"Playlist name\", \"Playlist pid\", \"Slice origin\"])\n",
    "keep_slices = random.sample(slices, 5)\n",
    "\n",
    "all_artists = np.array([], dtype='object')\n",
    "all_albums = np.array([], dtype='object')\n",
    "all_tracks = np.array([], dtype='object')\n",
    "\n",
    "for df_slice_item in tqdm(slices, desc=\"Loading slices...\"):\n",
    "\tdf_slice = pd.read_json(save_dir + df_slice_item)\n",
    "\t\t\t\n",
    "\tall_artists = np.append(all_artists, df_slice['Track artist'].unique())\n",
    "\tall_albums = np.append(all_albums, df_slice['Track album'].unique())\n",
    "\tall_tracks = np.append(all_tracks, df_slice['Track name'].unique())\n",
    "\n",
    "\tif df_slice_item in keep_slices:\n",
    "\t\tmain_df = pd.concat([main_df, df_slice])\n",
    "\n",
    "all_artists = set(all_artists)\n",
    "all_albums = set(all_albums)\n",
    "all_tracks = set(all_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['Playlist pid'] = main_df[\"Playlist pid\"].apply(lambda x: x.replace(\",\", \"\"))\n",
    "main_df = main_df.groupby(\"Playlist pid\")\n",
    "len(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = os.listdir(save_dir)\n",
    "df_tracks_album_relation_count = pd.DataFrame(columns=list(all_albums), dtype='int16')\n",
    "df_tracks_artist_relation_count = pd.DataFrame(columns=list(all_artists), dtype='int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for playlist_pid in tqdm(random.sample(list(main_df.groups.keys()), 1000), desc=\"Loading data...\"):\n",
    "\tplaylist = main_df.get_group(playlist_pid)\n",
    "\talbums = playlist['Track album'].value_counts()\n",
    "\tartists = playlist['Track artist'].value_counts()\n",
    "\tfor track in playlist['Track name'].unique():\n",
    "\t\tif not track in df_tracks_album_relation_count.index:\n",
    "\t\t\trow = pd.DataFrame(albums, dtype='int16').fillna(0).transpose()\n",
    "\t\t\trow.index = [track]\n",
    "\t\t\tstart_df = time.time()\n",
    "\t\t\tdf_tracks_album_relation_count = pd.concat([df_tracks_album_relation_count, row], axis=0)\n",
    "\t\t\tprint(\"%.3fs to concat\" % (time.time() - start_df))\n",
    "\t\t\tif_done = time.time()\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# if not track in df_tracks_artist_relation_count.index:\n",
    "\t\t# \trow = pd.DataFrame(artists, index=[track], ).transpose().fillna(0).astype('int16')\n",
    "\t\t# \tdf_tracks_artist_relation_count = pd.concat([df_tracks_artist_relation_count, row])\n",
    "\t\t# \tcontinue\n",
    "\n",
    "\t\tdf_tracks_album_relation_count.at[track] = df_tracks_album_relation_count.loc[track].add(albums, fill_value=0)\n",
    "\t\t# df_tracks_artist_relation_count.at[track] = df_tracks_artist_relation_count.loc[track].add(artists, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track in df_tracks_album_relation_count.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tracks_album_relation_count = pd.concat([df_tracks_album_relation_count, row], axis=0)\n",
    "df_tracks_album_relation_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [2, 3, 5]})\n",
    "df1.index = ['a', 'b', 'c']\n",
    "df2 = pd.DataFrame({\"C\": [2], \"D\": [2]})\n",
    "df2.index = ['d']\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hello there"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
