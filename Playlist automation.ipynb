{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BHT Data Applications project\n",
    "# Automatic Anime recommendation Algorithm\n",
    "### This project aims to create an algorithm that can determine what anime to recommend to a user.\n",
    "##### Authors: Rashmi Di Michino and Antonin Mathubert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 320000 users and 16000 animes dataset was taken from https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020 <br>\n",
    "We are going to use this dataset to build a model that can recommend an anime based on the animes that the user is watching, has dropped, has kept on hold or put on their watching list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing and parsing the data\n",
    "First, we want to import all of our available data in a suitable manner so it is treatable for the next steps of the project.<br><br>\n",
    "In order to load the data, we are going to do it by chunking the csv file so it's more efficient. Then we're changing the default type of the columns to be more convenient memory wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import cupy as cp\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chunks = pd.read_csv(\"dataset/anime/animelist.csv\", chunksize=10000)\n",
    "chunks = []\n",
    "for chunk in dataset_chunks:\n",
    "    chunks.append(chunk)\n",
    "    \n",
    "dataset = pd.concat(chunks, ignore_index=True)\n",
    "dataset = dataset.astype({'user_id': \"int32\", 'anime_id': 'int32', \"watching_status\": \"int16\"})\n",
    "\n",
    "dataset_chunks = None\n",
    "chunks = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chunks = pd.read_csv(\"C:/Users/rashm/OneDrive/Desktop/data_applications_project/julius/anime_dataset/animelist.csv\", chunksize=10000)\n",
    "\n",
    "chunks = []\n",
    "for chunk in dataset_chunks:\n",
    "    chunks.append(chunk)\n",
    "    \n",
    "dataset = pd.concat(chunks, ignore_index=True)\n",
    "dataset = dataset.astype({'user_id': \"int32\", 'anime_id': 'int32', \"watching_status\": \"int16\"})\n",
    "\n",
    "dataset_chunks = None\n",
    "chunks = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recommendation system based on the watched animes\n",
    "In this first version we're going to implement a recommendation system based on which animes the users have seen, for example if someone has watched cowboy bepop, they're going to be recommended to see death note\n",
    "#### Reducing the dataset\n",
    "As the dataset we're working with is too large, we're going to reduce it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['rating', 'watched_episodes'], axis=1, inplace=True)\n",
    "dataset = dataset[(dataset['anime_id'] < 10000) & (dataset['user_id'] < 20000)]\n",
    "dataset = dataset[(dataset['user_id'] != 61960) & (dataset['watching_status'] != 4)]\n",
    "dataset = dataset.drop(\"watching_status\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a sample of how the dataset is structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset.head(100))\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is pivoting the dataset: we're constructing a matrix that will be used to build the recommendation system, where the rows are the users' ids and the columns are the animes' ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.pivot(index='user_id', columns='anime_id', values='anime_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now converting our matrix into a binary matrix in order to be able to retrieve the association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset.notnull()] = True\n",
    "dataset = dataset.fillna(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are exploiting the mlxtend library to build the recommendation system and we're retrieving the association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets  = apriori(dataset, use_colnames=True, min_support=0.2)\n",
    "\n",
    "frequent_itemsets\n",
    "\n",
    "rules = association_rules(frequent_itemsets)\n",
    "\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running this next cell we can see that for the users that have seen Cowboy Bepop it's recommended to see Death Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: [x for x in x])\n",
    "value = 1535\n",
    "rules[rules[\"antecedents\"].apply(lambda x: value in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(dataset.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.nan_to_num(dataset, nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.where(dataset != 0, 1, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
