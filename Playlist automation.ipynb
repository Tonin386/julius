{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BHT Data Applications project\n",
    "# Automatic Anime recommendation Algorithm\n",
    "### This project aims to create an algorithm that can determine what anime to recommend to a user.\n",
    "##### Authors: Rashmi Di Michino and Antonin Mathubert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 320000 users and 16000 animes dataset was taken from https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020 <br>\n",
    "We are going to use this dataset to build a model that can recommend an anime based on the animes that the user is watching, has dropped, has kept on hold or put on their watching list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing and parsing the data\n",
    "First, we want to import all of our available data in a suitable manner so it is treatable for the next steps of the project.<br><br>\n",
    "In order to load the data, we are going to do it by chunking the csv file so it's more efficient. Then we're changing the default type of the columns to be more convenient memory wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/rashm/OneDrive/Desktop/data_applications_project/julius/anime_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/anime/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chunks = pd.read_csv(path+\"animelist.csv\", chunksize=20000)\n",
    "\n",
    "chunks = []\n",
    "for chunk in dataset_chunks:\n",
    "    chunks.append(chunk)\n",
    "    \n",
    "dataset = pd.concat(chunks, ignore_index=True)\n",
    "dataset = dataset.astype({'user_id': \"int32\", 'anime_id': 'int32', \"watching_status\": \"int16\"})\n",
    "\n",
    "dataset_chunks = None\n",
    "chunks = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recommendation system based on the watched animes\n",
    "In this first version we're going to implement a recommendation system based on which animes the users have seen, for example if someone has watched cowboy bepop, they're going to be recommended to see death note\n",
    "#### Reducing the dataset\n",
    "As the dataset we're working with is too large, we're going to reduce it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['watched_episodes'], axis=1, inplace=True)\n",
    "dataset = dataset[(dataset['anime_id'] < 10000) & (dataset['user_id'] < 35000)]\n",
    "dataset = dataset[(dataset['user_id'] != 61960) & (dataset['watching_status'] != 4)]\n",
    "dataset = dataset.drop(\"watching_status\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a sample of how the dataset is structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset.head(10))\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot the dataset\n",
    "The next step is pivoting the dataset: we're constructing a matrix that will be used to build the recommendation system, where the rows are the users' ids and the columns are the animes' ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.pivot(index='user_id', columns='anime_id', values='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now converting our matrix into a binary matrix in order to be able to retrieve the association rules: we only take into account the ratings that are above 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset > 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving the association rules\n",
    "Finally, we are exploiting the mlxtend library to build the recommendation system and we're retrieving the association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets  = apriori(dataset, use_colnames=True, min_support=0.1) #Getting under 0.1 support takes too much computation time / memory and lacks of meaning.\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: [x for x in x])\n",
    "rules[\"consequents\"] = rules[\"consequents\"].apply(lambda x: [x for x in x])\n",
    "rules = rules[rules[\"confidence\"] > 0.75].reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the rules detected by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rules.head(20))\n",
    "print(f\"{len(rules)} rules found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the rules\n",
    "These functions are designed to parse and filter the results of the detected rules, so we can understand them more easily.\n",
    "\n",
    "<hr>\n",
    "\n",
    "```find_recommendations_precise``` will compute every possible combination of the watched anime ids, and try to find them in the rules dataset.\n",
    "<hr>\n",
    "\n",
    "```find_recommendations_free``` will look for every occurence of each anime id in the rules, even if the antecedents frozen set isn't containing only the given id. \n",
    "\n",
    "It will return one dataset per seen anime, ordered from highest to lowest weight. Weight is computed by adding every confidence value obtained in the rules containing the recommendated anime in the consequents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes_df = pd.read_csv(path+\"anime.csv\")\n",
    "animes_df = animes_df[animes_df[\"MAL_ID\"] < 10000]\n",
    "\n",
    "def generate_combinations(ids):\n",
    "\tresult = []\n",
    "\tfor r in range(1, len(ids) + 1):\n",
    "\t\tpermutations = itertools.permutations(ids, r)\n",
    "\t\tfor p in permutations:\n",
    "\t\t\tresult.append(list(p))\n",
    "\n",
    "\tprint(f\"Found {len(result)} possible combinations.\")\n",
    "\treturn result\n",
    "\n",
    "def find_recommendations_precise(anime_ids):\n",
    "\trecommendations = []\n",
    "\t\n",
    "\tfor combination in tqdm(generate_combinations(anime_ids), desc=\"Trying every possible combination...\"):\n",
    "\t\tfilter_df = rules[\"antecedents\"].apply(lambda x: x == combination) & rules[\"consequents\"].apply(lambda x: np.all([id not in x for id in anime_ids]))\n",
    "\t\tif filter_df.apply(lambda x: x != False).sum() < 1:\n",
    "\t\t\tcontinue\n",
    "\t\trecommendation = (combination, rules[filter_df][\"consequents\"].values, rules[filter_df][\"confidence\"].values, rules[filter_df][\"lift\"].values)\n",
    "\t\trecommendations.append(recommendation)\n",
    "\n",
    "\treturn recommendations\n",
    "\n",
    "def find_recommendations_free(anime_ids):\n",
    "\trecommendations = []\n",
    "\n",
    "\tfor id in anime_ids:\n",
    "\t\tfilter_df = rules[\"antecedents\"].apply(lambda x: id in x) & rules[\"consequents\"].apply(lambda x: np.all([id not in x for id in anime_ids]))\n",
    "\t\tif filter_df.apply(lambda x: x != False).sum() < 1:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\trecommendation = pd.DataFrame({\"source\": id, \"antecedents\": rules[filter_df][\"antecedents\"].values, \"consequents\": rules[filter_df][\"consequents\"].values, \"confidence\": rules[filter_df][\"confidence\"].values, \"lift\": rules[filter_df][\"lift\"].values})\n",
    "\t\trecommendations.append(recommendation)\n",
    "\n",
    "\trecommendations = pd.concat(recommendations)\n",
    "\trecommendations_dict = {anime: {}  for anime in anime_ids}\n",
    "\trecommendations_df = []\n",
    "\tfor anime in recommendations_dict:\n",
    "\t\trows = recommendations[recommendations[\"source\"] == anime]\n",
    "\t\tfor _, row in rows.iterrows():\n",
    "\t\t\tfor x in row[\"consequents\"]:\n",
    "\t\t\t\tif x in recommendations_dict[anime]:\n",
    "\t\t\t\t\trecommendations_dict[anime][x] += row[\"confidence\"]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\trecommendations_dict[anime][x] = row[\"confidence\"]\n",
    "\t\t\n",
    "\t\tfor anime_recommended in recommendations_dict[anime]:\n",
    "\t\t\trecommendations_df.append([animes_df[animes_df[\"MAL_ID\"] == anime][\"Name\"].values[0], animes_df[animes_df[\"MAL_ID\"] == anime_recommended][\"Name\"].values[0], recommendations_dict[anime][anime_recommended]])\n",
    "\n",
    "\trecommendations_df = pd.DataFrame(recommendations_df, columns=['source', 'recommended_id', 'weight']).sort_values(by=\"weight\", ascending=False).groupby(\"source\")\n",
    "\n",
    "\treturn recommendations_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the previously defined function and parse the results to print them and link them with the anime infos dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_animes = [30, 32, 42, 43, 47, 1535] #More than 7 at a time takes forever.\n",
    "print(seen_animes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recommendations in find_recommendations_precise(seen_animes):\n",
    "\tfor i in range(len(recommendations[1])):\n",
    "\t\trecommendation = (recommendations[0], recommendations[1][i], recommendations[2][i], recommendations[3][i])\n",
    "\t\tprint(\"Because you have seen %s, we think you would like %s with %.3f%% confidence. You are also %.3f%% more likely to watch this/these anime(s).\" % (\n",
    "\t\t\t\" and \".join([animes_df[animes_df[\"MAL_ID\"] == x][\"Name\"].values[0] + f\" ({str(x)})\" for x in recommendation[0]]), \n",
    "\t\t\t\" and \".join([animes_df[animes_df[\"MAL_ID\"] == x][\"Name\"].values[0] + f\" ({str(x)})\" for x in recommendation[1]]), \n",
    "\t\t\trecommendation[2] * 100,\n",
    "\t\t\trecommendation[3] * 100 - 100)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part needs improvement, because the results are not consise enough. \n",
    "# We could create a list of every anime that has been watch by people who's seen a certain anime, and return the most common ones instead of every one. \n",
    "for index, recommendation_df in find_recommendations_free(seen_animes):\n",
    "\tdisplay(recommendation_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Graphs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
